{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ytaPiTOaAz7"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdhKR-Y7mzqV"
      },
      "source": [
        "from keras.layers import Dense,LSTM\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WApqqpagatXt"
      },
      "source": [
        "Read the data. Here I'm using the .txt file of my favorite book 'The Alchemist'.\n",
        "\n",
        "First, look in the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvqlq8bwaam5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51479c82-0c3a-40e7-825a-78e0c6f5a376"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open('alchemist.txt', 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 324423 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BYk1xhLa8V4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81742427-e3f0-48bc-f4dd-b5708c9c39a1"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Project Gutenberg EBook of The Alchemist, by Ben Jonson\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere at no cost and with\r\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
            "re-use it under the terms of the Project Gutenberg \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkJ_5C5ybOq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb34ead-afcc-4f57-97ff-a06e210d39b2"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap6LKRZvbYx1"
      },
      "source": [
        "Process the text\n",
        "\n",
        "Vectorize the text\n",
        "\n",
        "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlJwkz5RbdMw"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TJxiSQ1hhNW"
      },
      "source": [
        "The prediction task\n",
        "\n",
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task we're training the model to perform. The input to the model will be a sequence of characters, and we train the model to predict the outputâ€”the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n",
        "\n",
        "Create training examples and targets\n",
        "\n",
        "Next divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuF_yJGthnvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ecfab98-0e5f-4095-866d-6692e38c12e1"
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T\n",
            "h\n",
            "e\n",
            " \n",
            "P\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtyvL4vFiDW9"
      },
      "source": [
        "The batch method lets us easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDzAi9mmiG6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "642656e6-b633-462c-ed86-3425d482dc7a"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'The Project Gutenberg EBook of The Alchemist, by Ben Jonson\\r\\n\\r\\nThis eBook is for the use of anyone an'\n",
            "'ywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-'\n",
            "'use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gu'\n",
            "'tenberg.org\\r\\n\\r\\n\\r\\nTitle: The Alchemist\\r\\n\\r\\nAuthor: Ben Jonson\\r\\n\\r\\nRelease Date: May, 2003 [Etext #4081]\\r'\n",
            "'\\nPosting Date: January 7, 2010\\r\\n\\r\\nLanguage: English\\r\\n\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK TH'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lpa_PJ1kilMf"
      },
      "source": [
        "For each sequence, duplicate and shift it to form the input and target text by using the map method to apply a simple function to each batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Fz9l6xTimnV"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0TUAUR_i39T"
      },
      "source": [
        "Print the first examples input and target values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9loei1c-i5fT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f05a862-26db-4099-e4b2-19a8e761e470"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data:  'The Project Gutenberg EBook of The Alchemist, by Ben Jonson\\r\\n\\r\\nThis eBook is for the use of anyone a'\n",
            "Target data: 'he Project Gutenberg EBook of The Alchemist, by Ben Jonson\\r\\n\\r\\nThis eBook is for the use of anyone an'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTXfK1w6jtTg"
      },
      "source": [
        "Create training batches\n",
        "\n",
        "We used tf.data to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpe5_lsee4dg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "537a159f-9e5f-43b2-8f9c-1833da169f38"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYEKPJ-NkbSh"
      },
      "source": [
        "Build The Model\n",
        "\n",
        "Use tf.keras.Sequential to define the model. For this simple example three layers are used to define our model:\n",
        "\n",
        "tf.keras.layers.Embedding: The input layer. A trainable lookup table that will map the numbers of each character to a vector with embedding_dim dimensions;\n",
        "\n",
        "tf.keras.layers.GRU: A type of RNN with size units=rnn_units (You can also use a LSTM layer here.)\n",
        "\n",
        "tf.keras.layers.Dense: The output layer, with vocab_size outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHaiwI1_knUL"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP7pBhcmdJlQ"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Embedding(vocab_size,embedding_dim,batch_input_shape=[batch_size,None]),\n",
        "                            tf.keras.layers.GRU(rnn_units, return_sequences=True,  stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "                            tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "  return model"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWWluMcGeMY9"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzqB_vXQlsra"
      },
      "source": [
        "Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGTwvn-9mch1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1d85759-601c-40a4-aca1-2e529a38d8c5"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 86) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw4GhU0ImwZ_"
      },
      "source": [
        "In the above example the sequence length of the input is 100 but the model can be run on inputs of any length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxKU4npylbZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca983226-667a-46ce-a449-f2f58fddc55b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (64, None, 256)           22016     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (64, None, 86)            88150     \n",
            "=================================================================\n",
            "Total params: 4,048,470\n",
            "Trainable params: 4,048,470\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY9RA1IjnVPy"
      },
      "source": [
        "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop.\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klpi-b6ZnA4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec1e3ca-806c-45ad-c4c2-86c5f33e1fb6"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "#This gives us, at each timestep, a prediction of the next character index:\n",
        "sampled_indices"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([51, 60, 22, 62, 45, 76, 62, 25, 31, 26,  0, 61, 81, 30, 26, 11,  3,\n",
              "        1, 69, 59,  1, 44, 25, 52, 56, 27, 31, 18, 14, 27,  2, 24, 49, 65,\n",
              "       24, 56, 56, 37, 31, 14, 61, 69, 57, 40, 27, 26, 67, 21, 57, 10,  4,\n",
              "       49, 12, 24, 75, 39, 62, 54, 74,  4, 85,  8, 62,  1, 17, 20,  9, 40,\n",
              "       14,  5, 61, 39, 10, 28, 48, 61, 36, 33, 71, 13, 50, 84, 26, 15,  0,\n",
              "       34,  4, 62, 47, 64, 43, 69, 28, 22, 58, 25, 38, 78, 14, 43])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cpt66icn3LZ"
      },
      "source": [
        "Train the model\n",
        "\n",
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character.\n",
        "\n",
        "Attach an optimizer, and a loss function\n",
        "The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because our model returns logits, we need to set the from_logits flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL77odsVn7G4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d82e7d2d-9fe0-4af7-87c0-df4da2b873ad"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 86)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.4552207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAZQZedVoRzi"
      },
      "source": [
        "Configure the training procedure using the tf.keras.Model.compile method. We'll use tf.keras.optimizers.Adam with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROcK85vxoSGB"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wsLm3DcolOB"
      },
      "source": [
        "Configure checkpoints\n",
        "\n",
        "Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBjDBQ6iooRs"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqctwrjRoyol"
      },
      "source": [
        "Execute the training\n",
        "\n",
        "To keep training time reasonable, use 10 epochs to train the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4Xhz4YXo_WF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f67e8011-ffe5-436e-f6c5-05804b0663d0"
      },
      "source": [
        "EPOCHS=10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "50/50 [==============================] - 9s 130ms/step - loss: 3.5547 - accuracy: 0.1709\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 7s 128ms/step - loss: 2.5254 - accuracy: 0.3167\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 7s 127ms/step - loss: 2.2852 - accuracy: 0.3662\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 7s 126ms/step - loss: 2.1582 - accuracy: 0.3876\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 7s 127ms/step - loss: 2.0507 - accuracy: 0.4131\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 7s 128ms/step - loss: 1.9587 - accuracy: 0.4343\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 7s 128ms/step - loss: 1.8829 - accuracy: 0.4526\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 7s 127ms/step - loss: 1.8126 - accuracy: 0.4721\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 7s 131ms/step - loss: 1.7477 - accuracy: 0.4897\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 7s 128ms/step - loss: 1.6862 - accuracy: 0.5070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvTbxfT9qL02"
      },
      "source": [
        "Generate text\n",
        "\n",
        "Restore the latest checkpoint\n",
        "To keep this prediction step simple, use a batch size of 1.\n",
        "\n",
        "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
        "\n",
        "To run the model with a different batch_size, we need to rebuild the model and restore the weights from the checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgI-Ekp6qO9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31bed5e-1e3b-49b8-9a6d-c975fcffa602"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (1, None, 256)            22016     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (1, None, 86)             88150     \n",
            "=================================================================\n",
            "Total params: 4,048,470\n",
            "Trainable params: 4,048,470\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sydC-N4iqpfZ"
      },
      "source": [
        "The prediction loop\n",
        "\n",
        "The following code block generates the text:\n",
        "\n",
        "It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
        "\n",
        "Get the prediction distribution of the next character using the start string and the RNN state.\n",
        "\n",
        "Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
        "\n",
        "The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters.\n",
        "\n",
        "Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwxb5OPEqtgS"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 3000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # We pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rYFxSWjrEC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa0a641-706d-4d41-a0b1-697610490b98"
      },
      "source": [
        "print(generate_text(model, start_string=u\"egypt \"))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "egypt and evenblempinare them im thes tas hon,\r\n",
            "  You, by coust, se.\r\n",
            "\r\n",
            "  FACE. Nay, and  wee go Engive?\r\n",
            "\r\n",
            "   NAVE. You say, \"Thrizan That Sturk, sube,\r\n",
            "  Whe pleas.\r\n",
            "  That shis now!\r\n",
            "   (Geest the feargel\r\n",
            "(he  lang anding by bright thenedly Ase fene: them accoptake henes,\r\n",
            "  This was meatine. He his mest\r\n",
            "  Chere, a doct on:\r\n",
            "  Turm is berought, arl'do?\r\n",
            "\r\n",
            "  [AKATE.]\r\n",
            "\r\n",
            "  [EXIT DRUG.]\r\n",
            "\r\n",
            "  SUR. I huad inowar!\r\n",
            "\r\n",
            "  DRUG. Ron hil cheraining.\r\n",
            "  An prayk con?\r\n",
            "  Whas you give these fear greer goar ip an the deaces;\r\n",
            "  And'd me come that a candow, dewassix, nabue by cligtin,\r\n",
            "  And wast do), and Mad.\r\n",
            "\r\n",
            "  SUB. I sape hin not!\r\n",
            "  It your 2pponce with pladam\r\n",
            "  Shey have blears, so think you court\r\n",
            "  Oix of BALNOD DERCONIT MOOST.]\r\n",
            "\r\n",
            "  SUB. Adther.\r\n",
            "\r\n",
            "  SUR. No, \"hight,\r\n",
            "  And lated it is somperan.\r\n",
            "\r\n",
            "  A FACE. Thy inst ha, poetly: the gill:\r\n",
            "  I that us shojelatice of passies the ushally passe be dough\r\n",
            "  Which that?\r\n",
            "\r\n",
            "  ANA. Are you pees this bees, and hast, bath you prekand,\r\n",
            "  What you day the glaice; as and when.\r\n",
            "\r\n",
            "  FACE. Another, and gull pasting, in!\"  Come: wor be strange him.\r\n",
            "  At hougan.\r\n",
            "\r\n",
            "  3 trume a meathen; you coors no hos sleetes\r\n",
            "  The dease in marrabt\r\n",
            "  Of heath your coory at ant dood noug, be sped-,\r\n",
            "  I all thes not you good bean mean some trampt;\r\n",
            "  Has the count it way hand game?\r\n",
            "\r\n",
            "  Don Weaks\r\n",
            "  Thing shall terms, sumporiat mingerm you Broject Gune\r\n",
            "  You camil. Yous have have then.\r\n",
            "\r\n",
            "  SUB. Hay shell hol!\r\n",
            "  Abot the woskny in the she with it?\r\n",
            "  I'll so would lo to cur the varret; sir:\r\n",
            "  You mack do you peff the umpmys, de buther's.\r\n",
            "\r\n",
            "  K if virnome with shently.\r\n",
            "  [EXITID FACE.].  The we this gell; ane will but hed\r\n",
            "  And \"Jonson's plose\r\n",
            "distablated parstingleborid ot this orre of the\r\n",
            "cabrut of CKAMTHIN, if the forthoug in obon,\r\n",
            "  Cory, they mat him tion.\r\n",
            "\r\n",
            "  DOL. Ay, shall were you not a termond; lesonk!\r\n",
            "\r\n",
            "  hair ow hale be aw'S; the trunuss,\r\n",
            "  Be thes,\r\n",
            "  Thiin do you was an--waken speaks toor.\r\n",
            "\r\n",
            "--TROUBSE-Hr take gools.\r\n",
            "\r\n",
            "LONE, tradge. If willly racivy he\r\n",
            "docestian or the right of this grang as this agroman or painisted\r\n",
            "of entagte us he'd andorg you have no orver\r\n",
            "\"  The seretine, but we lit how, the\r\n",
            "Rearation he was\" whither mumate.\r\n",
            "\r\n",
            "FUM, to fole\r\n",
            "  In peays do thus I geal speik you good ow shistive cointers;\r\n",
            "  In heard him no mory in a curtus, and all\r\n",
            "bestorght, exc. Ho docy\r\n",
            "  Y have been this fay this, I have you till ther.\r\n",
            "\r\n",
            "TRRLY, foor.\r\n",
            "\r\n",
            "WAN, peenity, ashipl sheppe cop.\r\n",
            "\r\n",
            "PRIAR, enfus\r\n",
            "Sholed, shall dearting or the puints, to montars is their her, that the presert\r\n",
            "wimacon afficious quarre is poracez-frace.\r\n",
            "\r\n",
            "PUNTY, court beind by pees/;\r\n",
            "perfuied); will whis now we the baske;\r\n",
            "  And chatch whe sturil sic;\r\n",
            "  The preselin by this with the\r\n",
            "  This to gheem, pingle.\r\n",
            "\r\n",
            "  MAM. O, the surous virours:\r\n",
            "  Of the the just, and the dorach, or certain to be rackmen;\r\n",
            "  In your fith dimical.\r\n",
            "\r\n",
            "  FACE. 'T, if all ter,\r\n",
            "  Our %i'll stall seet castan, their child.\r\n",
            "\r\n",
            "  2 NEI. Hol gramon.\r\n",
            "\r\n",
            "  SUB. Enou\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6RygAlvgyPY"
      },
      "source": [
        "###**Tuning the hyperparameters:**\n",
        "##### Epochs *from* 10 to 25,\n",
        " ##### Batch Size *from* 64 to 128,\n",
        " ##### Learning Rate *from* 0.001 to 0.01, \n",
        " ##### Adding LSTM layer instead of GRU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVYtHmwzgrhj"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                         activation='tanh', recurrent_activation='sigmoid',\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaILUWXniRlx",
        "outputId": "2cfdb6a0-341c-4ba5-ad27-1ccd0d92f3f7"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=128) #increased batchsize"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtAdSzVbiW8D",
        "outputId": "03102e1c-8a48-4da8-8f19-86d1a0690371"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (128, None, 256)          22016     \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (128, None, 1024)         5246976   \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (128, None, 1024)         0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (128, None, 86)           88150     \n",
            "=================================================================\n",
            "Total params: 5,357,142\n",
            "Trainable params: 5,357,142\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ-8EBmqwMUW",
        "outputId": "e5d70bb2-ed23-4719-adf4-92f97705686d"
      },
      "source": [
        "dataset= sequences.map(split_input_target)\n",
        "dataset= dataset.shuffle(10000).batch(128, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((128, 100), (128, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c33pK51xkNwf"
      },
      "source": [
        "adam = Adam (learning_rate=0.01)\n",
        "model.compile(optimizer=adam, loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5QDZ-n6i4sh",
        "outputId": "fbd581f8-c5b0-422e-ab30-2200e6f431df"
      },
      "source": [
        "EPOCHS= 25\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "25/25 [==============================] - 9s 259ms/step - loss: 3.9494 - accuracy: 0.1481\n",
            "Epoch 2/25\n",
            "25/25 [==============================] - 7s 259ms/step - loss: 2.5285 - accuracy: 0.3133\n",
            "Epoch 3/25\n",
            "25/25 [==============================] - 7s 254ms/step - loss: 2.2185 - accuracy: 0.3758\n",
            "Epoch 4/25\n",
            "25/25 [==============================] - 7s 255ms/step - loss: 2.0611 - accuracy: 0.4106\n",
            "Epoch 5/25\n",
            "25/25 [==============================] - 7s 255ms/step - loss: 1.9469 - accuracy: 0.4386\n",
            "Epoch 6/25\n",
            "25/25 [==============================] - 7s 260ms/step - loss: 1.8546 - accuracy: 0.4602\n",
            "Epoch 7/25\n",
            "25/25 [==============================] - 7s 256ms/step - loss: 1.7822 - accuracy: 0.4782\n",
            "Epoch 8/25\n",
            "25/25 [==============================] - 7s 257ms/step - loss: 1.7238 - accuracy: 0.4933\n",
            "Epoch 9/25\n",
            "25/25 [==============================] - 7s 256ms/step - loss: 1.6708 - accuracy: 0.5077\n",
            "Epoch 10/25\n",
            "25/25 [==============================] - 7s 264ms/step - loss: 1.6291 - accuracy: 0.5186\n",
            "Epoch 11/25\n",
            "25/25 [==============================] - 7s 260ms/step - loss: 1.5889 - accuracy: 0.5296\n",
            "Epoch 12/25\n",
            "25/25 [==============================] - 7s 260ms/step - loss: 1.5557 - accuracy: 0.5387\n",
            "Epoch 13/25\n",
            "25/25 [==============================] - 7s 261ms/step - loss: 1.5230 - accuracy: 0.5472\n",
            "Epoch 14/25\n",
            "25/25 [==============================] - 7s 261ms/step - loss: 1.4939 - accuracy: 0.5539\n",
            "Epoch 15/25\n",
            "25/25 [==============================] - 7s 263ms/step - loss: 1.4656 - accuracy: 0.5620\n",
            "Epoch 16/25\n",
            "25/25 [==============================] - 7s 254ms/step - loss: 1.4382 - accuracy: 0.5691\n",
            "Epoch 17/25\n",
            "25/25 [==============================] - 7s 256ms/step - loss: 1.4134 - accuracy: 0.5760\n",
            "Epoch 18/25\n",
            "25/25 [==============================] - 7s 261ms/step - loss: 1.3904 - accuracy: 0.5812\n",
            "Epoch 19/25\n",
            "25/25 [==============================] - 7s 260ms/step - loss: 1.3656 - accuracy: 0.5885\n",
            "Epoch 20/25\n",
            "25/25 [==============================] - 7s 263ms/step - loss: 1.3458 - accuracy: 0.5940\n",
            "Epoch 21/25\n",
            "25/25 [==============================] - 7s 256ms/step - loss: 1.3261 - accuracy: 0.5995\n",
            "Epoch 22/25\n",
            "25/25 [==============================] - 7s 260ms/step - loss: 1.3038 - accuracy: 0.6044\n",
            "Epoch 23/25\n",
            "25/25 [==============================] - 7s 256ms/step - loss: 1.2831 - accuracy: 0.6112\n",
            "Epoch 24/25\n",
            "25/25 [==============================] - 7s 260ms/step - loss: 1.2674 - accuracy: 0.6149\n",
            "Epoch 25/25\n",
            "25/25 [==============================] - 7s 263ms/step - loss: 1.2477 - accuracy: 0.6208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iIIDC4Di47G",
        "outputId": "3114f5ff-93d3-41d4-86b7-baed4b226bc5"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (1, None, 256)            22016     \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (1, None, 1024)           0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (1, None, 86)             88150     \n",
            "=================================================================\n",
            "Total params: 5,357,142\n",
            "Trainable params: 5,357,142\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIn25IaQqbHx",
        "outputId": "8d88366e-9370-4a8c-ae4d-48a0f948d682"
      },
      "source": [
        "print(generate_text(model, start_string=u\"egypt \"))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "egypt of all, in Hebre of the employed to enfere.\r\n",
            "\r\n",
            "TIRD, assonation.\r\n",
            "\r\n",
            "PATURM, greated stope of a parts.\r\n",
            "\r\n",
            "BRADKMOUS, smitte.\r\n",
            "\r\n",
            "SHIGLLINE, pupply.\r\n",
            "\r\n",
            "EANIANS, untimate.\r\n",
            "\r\n",
            "FUCIPING, attendune, continuenated.\r\n",
            "\r\n",
            "JAMPULDER, conception.\r\n",
            "\r\n",
            "GROPE, whief.\r\n",
            "\r\n",
            "EDGE, worn and conge.\r\n",
            "\r\n",
            "MORRIST, reverse with the widown, a femf\r\n",
            "what call like a round of promusting in master\r\n",
            "  An mistress for your brother o'er he is very up. all my blazoner,\r\n",
            "  To say my count help, and these thousand back. I'll do Drydle\r\n",
            "  DOL. Not the will: you are to make you to keep\r\n",
            "  No brave a sight, my heart of the best is that\r\n",
            "courcual colours to go acsoritation or right.\r\n",
            "\r\n",
            "  DAP. What, good sure it of a ring. Plays it?\r\n",
            "\r\n",
            "  RUB. You found, then sui, Pissill'd, Ananias of the sister\r\n",
            "  That discitable Nehend the worod of read person play\r\n",
            "from their\r\n",
            "will use of leplet alont of the saining or transanted creagrie.\r\n",
            "\r\n",
            "DIGHTLE, relation.\r\n",
            "\r\n",
            "BRAGANT, repair of caprice.\r\n",
            "\r\n",
            "FACUL, becoverns more from sublimatice-vocour.\r\n",
            "\r\n",
            "DISTART, wonder.\r\n",
            "\r\n",
            "GANGANROLLY, I drught, scour\" supprese.\r\n",
            "\r\n",
            "SAST-SUB, blong.\r\n",
            "\r\n",
            "TDANCE, prospleted.\r\n",
            "\r\n",
            "RELOURED, umparent.\r\n",
            "\r\n",
            "EXORPAIN, feart, was unttle for a famous\r\n",
            "at used from the less paised which one's own, master plays abby\r\n",
            "considyer, where mean i'll particular cockissis directive to make come to Deking Jonson's\r\n",
            "monal.\r\n",
            "\r\n",
            "CACK (Spase, want (on.\r\n",
            "\r\n",
            "EXHALE, heads send malter peesord proceed.\r\n",
            "\r\n",
            "FAINSUB, fetch, tricks.\r\n",
            "\r\n",
            "  FACE. I dut for your tinct man,\r\n",
            "  Would my unco'taster than can our worship; but it, master call\r\n",
            "   Jonson\r\n",
            "   Set When donations.  When afiet.\r\n",
            "\r\n",
            "SURVERTISED, dass, wase.\r\n",
            "\r\n",
            "FAVERY, \"thought cockives.\r\n",
            "\r\n",
            "PELLOW (OR OR IN LOUDED, formine whine ane work.  Let Jonson, and Archive Foundate\r\n",
            "  Come, I dall make sent macher.\r\n",
            "\r\n",
            "  MAM. Good!\r\n",
            "\r\n",
            "  FACE. And a satis all.\r\n",
            "\r\n",
            "In; that mein In yar me, Piss;\r\n",
            "  And make her to-yrain's house, I'll have away.\r\n",
            "  I have many me' only you.\r\n",
            "  And zeak and hard for a Turk to-day as a shirt a whies.\r\n",
            "  I am no buy the lent te a hunger-count,\r\n",
            "  And take its up, a feaster,\r\n",
            "  And a ou ridi now, if you, sir.\r\n",
            "\r\n",
            "  MAM. LOWN, clord; put in exampes for ackets or sous distingument.\r\n",
            "\r\n",
            "ADEND CROLLOW, please.\r\n",
            "\r\n",
            "DISCOURSED, degood.\r\n",
            "\r\n",
            "ADALANT, reputing ooch; avendit\r\n",
            "and mogues); (?) Tale's affection and what war in occause,\r\n",
            "  And that I will be salep?\r\n",
            "\r\n",
            "  FACE [ASIDE]. LISbasy!\r\n",
            "  O, the pass knightman, the will year douse as that shall yep.\r\n",
            "\r\n",
            "  MAM. Very i'll come. Is't not short-deek, by a could\r\n",
            "  Of same of the that what passible such merculation of playing the presen\r\n",
            "  Strange at 'tis my wrof one thing: which e'er cays o' sure\r\n",
            "  Why Chapt first worship, notiquon, scan gold corrice boy, fear turn\r\n",
            "  Is electronic well son live.  It may\r\n",
            "an idee less states what, and sent one pedsons\r\n",
            "  Is e hard long, noce of 'emped,\r\n",
            "  Uses, beries gold to quarrels to adversay,nL, guft, in\r\n",
            "crack try with; with only not one speech, and many piuse is a worn cut nower and heapodies.  In\r\n",
            "affected the word known conse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIr8HJjZ06Hg"
      },
      "source": [
        "Here, the model did a decent job in generating valid text with accuracy of 62%. Hence, I'll add one more layer of LSTM with a couple of activation functions to tune it furthermore. Here I used tanh and sigmoid functions as activation and recurrent_activation functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oovAJo63rjN3"
      },
      "source": [
        "### **Tuning furthermore..**\n",
        "##### Epochs *from* 25 to 50,\n",
        " ##### Batch Size *from* 128 to 64,\n",
        " ##### Learning Rate *to its default*, \n",
        " ##### Adding one more layer of LSTM\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-kpi_4Fr2WH"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                         activation='tanh', recurrent_activation='sigmoid',\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                         activation='tanh', recurrent_activation='sigmoid',\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y09zmsuTtK26"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=64) #decreased batchsize"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiOe26HAtVPm",
        "outputId": "16e09744-d871-4b58-e99f-edd152182391"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (64, None, 256)           22016     \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (64, None, 1024)          0         \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (64, None, 1024)          8392704   \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (64, None, 1024)          0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (64, None, 86)            88150     \n",
            "=================================================================\n",
            "Total params: 13,749,846\n",
            "Trainable params: 13,749,846\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWegNEFYtZXb"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fsw0nCmfxh7t",
        "outputId": "3cd362ae-416a-47c5-8d8a-6a5da8108095"
      },
      "source": [
        "dataset= sequences.map(split_input_target)\n",
        "dataset= dataset.shuffle(10000).batch(64, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ll6oQkQtlYi",
        "outputId": "f4cfa578-8f55-467d-80ab-782127bd368b"
      },
      "source": [
        "EPOCHS= 50\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "50/50 [==============================] - 22s 354ms/step - loss: 3.5259 - accuracy: 0.1500\n",
            "Epoch 2/50\n",
            "50/50 [==============================] - 18s 356ms/step - loss: 2.8848 - accuracy: 0.2348\n",
            "Epoch 3/50\n",
            "50/50 [==============================] - 19s 358ms/step - loss: 2.4658 - accuracy: 0.3207\n",
            "Epoch 4/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 2.2301 - accuracy: 0.3677\n",
            "Epoch 5/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 2.0903 - accuracy: 0.4001\n",
            "Epoch 6/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 1.9807 - accuracy: 0.4278\n",
            "Epoch 7/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 1.8830 - accuracy: 0.4519\n",
            "Epoch 8/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 1.8014 - accuracy: 0.4724\n",
            "Epoch 9/50\n",
            "50/50 [==============================] - 19s 361ms/step - loss: 1.7265 - accuracy: 0.4937\n",
            "Epoch 10/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 1.6621 - accuracy: 0.5100\n",
            "Epoch 11/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 1.6015 - accuracy: 0.5274\n",
            "Epoch 12/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 1.5467 - accuracy: 0.5417\n",
            "Epoch 13/50\n",
            "50/50 [==============================] - 19s 361ms/step - loss: 1.4929 - accuracy: 0.5560\n",
            "Epoch 14/50\n",
            "50/50 [==============================] - 19s 361ms/step - loss: 1.4429 - accuracy: 0.5694\n",
            "Epoch 15/50\n",
            "50/50 [==============================] - 19s 361ms/step - loss: 1.3911 - accuracy: 0.5836\n",
            "Epoch 16/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 1.3399 - accuracy: 0.5972\n",
            "Epoch 17/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 1.2888 - accuracy: 0.6129\n",
            "Epoch 18/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 1.2346 - accuracy: 0.6292\n",
            "Epoch 19/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 1.1812 - accuracy: 0.6446\n",
            "Epoch 20/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 1.1247 - accuracy: 0.6624\n",
            "Epoch 21/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 1.0718 - accuracy: 0.6791\n",
            "Epoch 22/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 1.0154 - accuracy: 0.6982\n",
            "Epoch 23/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 0.9600 - accuracy: 0.7160\n",
            "Epoch 24/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 0.9041 - accuracy: 0.7356\n",
            "Epoch 25/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 0.8527 - accuracy: 0.7522\n",
            "Epoch 26/50\n",
            "50/50 [==============================] - 19s 361ms/step - loss: 0.7988 - accuracy: 0.7701\n",
            "Epoch 27/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 0.7530 - accuracy: 0.7852\n",
            "Epoch 28/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 0.7055 - accuracy: 0.8022\n",
            "Epoch 29/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 0.6608 - accuracy: 0.8163\n",
            "Epoch 30/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 0.6236 - accuracy: 0.8295\n",
            "Epoch 31/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 0.5847 - accuracy: 0.8430\n",
            "Epoch 32/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 0.5529 - accuracy: 0.8529\n",
            "Epoch 33/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 0.5222 - accuracy: 0.8639\n",
            "Epoch 34/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 0.4930 - accuracy: 0.8733\n",
            "Epoch 35/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 0.4695 - accuracy: 0.8806\n",
            "Epoch 36/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 0.4462 - accuracy: 0.8878\n",
            "Epoch 37/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 0.4279 - accuracy: 0.8934\n",
            "Epoch 38/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 0.4133 - accuracy: 0.8990\n",
            "Epoch 39/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 0.3998 - accuracy: 0.9027\n",
            "Epoch 40/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 0.3842 - accuracy: 0.9070\n",
            "Epoch 41/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 0.3723 - accuracy: 0.9107\n",
            "Epoch 42/50\n",
            "50/50 [==============================] - 19s 361ms/step - loss: 0.3594 - accuracy: 0.9145\n",
            "Epoch 43/50\n",
            "50/50 [==============================] - 19s 358ms/step - loss: 0.3510 - accuracy: 0.9167\n",
            "Epoch 44/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 0.3431 - accuracy: 0.9193\n",
            "Epoch 45/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 0.3334 - accuracy: 0.9216\n",
            "Epoch 46/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 0.3265 - accuracy: 0.9240\n",
            "Epoch 47/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 0.3178 - accuracy: 0.9262\n",
            "Epoch 48/50\n",
            "50/50 [==============================] - 19s 359ms/step - loss: 0.3120 - accuracy: 0.9278\n",
            "Epoch 49/50\n",
            "50/50 [==============================] - 19s 361ms/step - loss: 0.3063 - accuracy: 0.9288\n",
            "Epoch 50/50\n",
            "50/50 [==============================] - 19s 360ms/step - loss: 0.3023 - accuracy: 0.9306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeFdNZGwypV7",
        "outputId": "9ac96013-b8cc-45ad-ab3d-6f996d15aee6"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (1, None, 256)            22016     \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (1, None, 1024)           0         \n",
            "_________________________________________________________________\n",
            "lstm_13 (LSTM)               (1, None, 1024)           8392704   \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (1, None, 1024)           0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (1, None, 86)             88150     \n",
            "=================================================================\n",
            "Total params: 13,749,846\n",
            "Trainable params: 13,749,846\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCc2G791zLzd",
        "outputId": "c26e474e-b316-425c-e98a-20c0c3f62882"
      },
      "source": [
        "print(generate_text(model, start_string=u\"egypt \"))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "egypt that we do not necessarily\r\n",
            "knowledger, Jonson's fellow, cheatmelousing and player says set\r\n",
            "art in the plot us they sure.\r\n",
            "\r\n",
            "  KAS. I will not.\r\n",
            "\r\n",
            "  ANA. What is't you, sir?\r\n",
            "\r\n",
            "  ANA. Locusts\r\n",
            "  Of the fear hath placed him, and then all here have incidenels,\r\n",
            "  They say, a wear doublets, term another, playwright\r\n",
            "the letter do not solicit donations in locations\r\n",
            "where we have not received written confirmation of compliance.  Jonson's birthpipance\r\n",
            "ox the state of carrs of letters for puppets, with a\r\n",
            "company made up of the choicest spirits of himself, and if at no court not in strizen, and for a widow,\r\n",
            "  To furnish household.\r\n",
            "\r\n",
            "  SUB. Excellent, more live; and for mind\r\n",
            "  The sole of huro, and grieve me. The house, sir, does shop landuades spend that the\r\n",
            "baws duch the satire of \"Every Man Out of His Humour,\" and \"Cynthia's Revels,\"\r\n",
            "Daniel under the characters Facenberg, sir.\r\n",
            "\r\n",
            "  MAM. Will not you quit her in the house.\r\n",
            "\r\n",
            "  SUB. How, Dol!\r\n",
            "\r\n",
            "  FACE. She lies,\r\n",
            "  This is my faith, to call him officers with snuss and shown to gold,\r\n",
            "  And then fa, to-morrow his Devens; and these of Jonson's own.  It is\r\n",
            "in \"Bartholomew Fair\" that we are presented in the personal raff of\r\n",
            "which leave Revels, 4to, 1611;\r\n",
            "   Poetrap Wars and English. But there you had had the stone with this?\r\n",
            "\r\n",
            "  MAM. No, I do not trust me.\r\n",
            "\r\n",
            "  [EXIT.]\r\n",
            "\r\n",
            "  SUR. What need you? No, sir?\r\n",
            "\r\n",
            "  3 NEI. Yes, and toward so weeks,\r\n",
            "  Just of your stands your worship!\r\n",
            "  I sance you so?\r\n",
            "\r\n",
            "  KAS. It goes like law-French,\r\n",
            "  And that, the parts of Sarace; I will know hin state.\r\n",
            "\r\n",
            "  MAM. He is not strange. I have spoken it.\r\n",
            "\r\n",
            "  TRI. Let me find grace, sir, in your eyes; the man\r\n",
            "  I word.\r\n",
            "\r\n",
            "THREE-FARTHINGS, piece of silver current under Elizabeth.\r\n",
            "\r\n",
            "The last pease for not my roof\r\n",
            "  Over us still, and will not be numbered.\r\n",
            "\r\n",
            "  KAS. Od, you dog lose\r\n",
            "  As he was fetch in the only finest time.--He will have it so.\r\n",
            "\r\n",
            "  DAP. And will I tell you?\r\n",
            "\r\n",
            "  DAP. Yes, sir.\r\n",
            "\r\n",
            "  SUB. And pray you let me speak with them.\r\n",
            "\r\n",
            "  [ENTER SUBTLE, DISGUISED, AND OFTIS THE AND KANT.]\r\n",
            "\r\n",
            "  SUB. Mum, you must know not you well.\r\n",
            "  He will be gone.\r\n",
            "\r\n",
            "  KAS. I will:\r\n",
            "  --What she is he? How!\r\n",
            "\r\n",
            "  DOL. Yes; and the whose was sickness?\r\n",
            "  Do, sir, you see your use must have stear thir, sir.\r\n",
            "\r\n",
            "  [EXEUNT.]\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "ACT 2. SCENE 2.1.\r\n",
            "\r\n",
            "  AN OUTER ROOM IN LOVITHOUT.]\r\n",
            "  How now!\r\n",
            "\r\n",
            "  SUB. Ay, the same\r\n",
            "  For her your honour to your worship, that I shall be.\r\n",
            "  [TA THE THO FATE.]\r\n",
            "  You have had,\r\n",
            "  For the instruments, as from conserve,\r\n",
            "  With of the seven spheres, and clumims,\r\n",
            "  Before your hungrest comes here.\r\n",
            "  And I will eat the most equal sugger.\r\n",
            "\r\n",
            "  SUB. Agreed.\r\n",
            "\r\n",
            "  SUB. No, your clothes.--\r\n",
            "  Thou vermin, have I cannot be read by\r\n",
            "your equipment.\r\n",
            "\r\n",
            "1.F.  The Project Gutenberg Literary Archive Foundation at the Great Mideat Deven Man in His Humour\"\r\n",
            "there is certainly a caricature of the Uupigames, jetter on from\r\n",
            "the lines, charity, and poingess, presently, on his oldical six and brokend you disco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SIiK5YL3XKo"
      },
      "source": [
        "This time, the model did an excellent job in generating the exact text with an accuracy of 93%."
      ]
    }
  ]
}